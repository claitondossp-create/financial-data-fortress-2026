# FASE 1: ETL - Extra√ß√£o, Transforma√ß√£o e Carga de Dados Financeiros

> **Documenta√ß√£o T√©cnica Completa**  
> Pipeline de Processamento de Dados com Python e Pandas

---

## üìö √çndice

1. [Fundamentos Te√≥ricos do ETL](#fundamentos-te√≥ricos-do-etl)
2. [Arquitetura do Pipeline](#arquitetura-do-pipeline)
3. [Etapa 1: Extra√ß√£o de Dados](#etapa-1-extra√ß√£o-de-dados)
4. [Etapa 2: Limpeza e Valida√ß√£o](#etapa-2-limpeza-e-valida√ß√£o)
5. [Etapa 3: Transforma√ß√£o de Tipos](#etapa-3-transforma√ß√£o-de-tipos)
6. [Etapa 4: Feature Engineering](#etapa-4-feature-engineering)
7. [Etapa 5: Valida√ß√£o de Qualidade](#etapa-5-valida√ß√£o-de-qualidade)
8. [Otimiza√ß√£o e Performance](#otimiza√ß√£o-e-performance)
9. [Tratamento de Erros](#tratamento-de-erros)
10. [Boas Pr√°ticas e Troubleshooting](#boas-pr√°ticas-e-troubleshooting)

---

## 1. Fundamentos Te√≥ricos do ETL

### 1.1 O Que √â ETL?

**ETL (Extract, Transform, Load)** √© um padr√£o arquitetural de integra√ß√£o de dados que envolve tr√™s processos sequenciais:

1. **Extract (Extra√ß√£o)**: Leitura de dados de uma ou m√∫ltiplas fontes
2. **Transform (Transforma√ß√£o)**: Limpeza, valida√ß√£o e reestrutura√ß√£o dos dados
3. **Load (Carga)**: Armazenamento dos dados processados em um destino

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   EXTRACT    ‚îÇ ‚îÄ‚îÄ‚îÄ> ‚îÇ  TRANSFORM   ‚îÇ ‚îÄ‚îÄ‚îÄ> ‚îÇ     LOAD     ‚îÇ
‚îÇ              ‚îÇ      ‚îÇ              ‚îÇ      ‚îÇ              ‚îÇ
‚îÇ CSV, JSON,   ‚îÇ      ‚îÇ Limpeza      ‚îÇ      ‚îÇ Database,    ‚îÇ
‚îÇ APIs, DB     ‚îÇ      ‚îÇ Valida√ß√£o    ‚îÇ      ‚îÇ Data Lake,   ‚îÇ
‚îÇ              ‚îÇ      ‚îÇ Enriquecimento‚îÇ     ‚îÇ Warehouse    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.2 Por Que ETL √â Essencial?

**Problemas que o ETL Resolve**:

| Problema                      | Impacto                             | Solu√ß√£o ETL                    |
| ----------------------------- | ----------------------------------- | ------------------------------ |
| **Dados Sujos**               | An√°lises incorretas, decis√µes ruins | Limpeza e valida√ß√£o autom√°tica |
| **Formatos Inconsistentes**   | Impossibilidade de an√°lise agregada | Normaliza√ß√£o de tipos          |
| **Valores Faltantes**         | Gaps na an√°lise temporal            | Imputa√ß√£o inteligente          |
| **Dados Duplicados**          | Contagem inflacionada de m√©tricas   | Deduplica√ß√£o com hash/ID       |
| **Colunas Pouco Expressivas** | Dificuldade em extrair insights     | Feature Engineering            |

### 1.3 Por Que Pandas?

**Pandas** √© a biblioteca Python de facto para manipula√ß√£o de dados tabulares:

- **Performance**: Opera√ß√µes vetorizadas em C (at√© 100x mais r√°pido que loops Python)
- **Expressividade**: Sintaxe declarativa similar a SQL
- **Integra√ß√£o**: Suporta CSV, Excel, SQL, JSON, Parquet, etc.
- **Mem√≥ria Eficiente**: Otimiza√ß√µes como `category` dtype e chunking

**Alternativas**:

- **Dask**: Para datasets > 10GB (paraleliza√ß√£o)
- **Polars**: Para performance extrema (Rust-based)
- **PySpark**: Para ambientes distribu√≠dos (Hadoop/Spark)

---

## 2. Arquitetura do Pipeline

### 2.1 Vis√£o Geral do Fluxo de Dados

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   PIPELINE ETL - VIS√ÉO MACRO                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

INPUT: Financials.csv (121KB, 701 linhas, 16 colunas)
   ‚îÇ
   ‚îú‚îÄ> ETAPA 1: EXTRA√á√ÉO
   ‚îÇ   ‚îú‚îÄ pd.read_csv() com encoding UTF-8
   ‚îÇ   ‚îú‚îÄ An√°lise explorat√≥ria inicial (.info(), .describe())
   ‚îÇ   ‚îî‚îÄ Detec√ß√£o de tipos de dados (dtype inference)
   ‚îÇ
   ‚îú‚îÄ> ETAPA 2: LIMPEZA
   ‚îÇ   ‚îú‚îÄ Remo√ß√£o de espa√ßos em branco (.strip())
   ‚îÇ   ‚îú‚îÄ Deduplica√ß√£o (.drop_duplicates())
   ‚îÇ   ‚îú‚îÄ Tratamento de nulos (.fillna())
   ‚îÇ   ‚îî‚îÄ Normaliza√ß√£o de strings (.lower(), .replace())
   ‚îÇ
   ‚îú‚îÄ> ETAPA 3: TRANSFORMA√á√ÉO DE TIPOS
   ‚îÇ   ‚îú‚îÄ Convers√£o monet√°ria: "$1,234.56" ‚Üí 1234.56 (float64)
   ‚îÇ   ‚îú‚îÄ Convers√£o de datas: "01/06/2014" ‚Üí datetime64
   ‚îÇ   ‚îú‚îÄ Otimiza√ß√£o de mem√≥ria: object ‚Üí category
   ‚îÇ   ‚îî‚îÄ Valida√ß√£o de ranges (ex: pre√ßos > 0)
   ‚îÇ
   ‚îú‚îÄ> ETAPA 4: FEATURE ENGINEERING
   ‚îÇ   ‚îú‚îÄ M√©tricas derivadas (Margin, Discount Rate, ROI)
   ‚îÇ   ‚îú‚îÄ Agrega√ß√µes temporais (Quarter, Year-Quarter)
   ‚îÇ   ‚îú‚îÄ Binning categ√≥rico (Volume_Category)
   ‚îÇ   ‚îî‚îÄ Flags booleanas (is_profitable, has_discount)
   ‚îÇ
   ‚îú‚îÄ> ETAPA 5: VALIDA√á√ÉO DE QUALIDADE
   ‚îÇ   ‚îú‚îÄ Testes de integridade (Sales = Gross Sales - Discounts)
   ‚îÇ   ‚îú‚îÄ An√°lise de outliers (IQR method)
   ‚îÇ   ‚îú‚îÄ Verifica√ß√£o de valores negativos
   ‚îÇ   ‚îî‚îÄ Gera√ß√£o de relat√≥rio de qualidade
   ‚îÇ
   ‚îî‚îÄ> OUTPUT: Financials_Processado.csv (enriquecido com 6 colunas)
```

### 2.2 Estrutura de Dados do Arquivo Original

**Financials.csv - Schema**:

```python
Index | Column Name           | Dtype   | Sample Values                | Observa√ß√µes
------+-----------------------+---------+------------------------------+--------------------------
0     | Segment               | object  | "Government", "Midmarket"    | 5 categorias √∫nicas
1     | Country               | object  | "Canada", "France"           | 5 pa√≠ses
2     | Product               | object  | "Carretera", "Paseo"         | 6 produtos
3     | Discount Band         | object  | "None", "Low", "Medium"      | 4 n√≠veis + None
4     | Units Sold            | object  | " $1,618.50 "                | String com $, ,
5     | Manufacturing Price   | float   | 3.00, 10.00, 120.00          | J√° num√©rico
6     | Sale Price            | float   | 20.00, 125.00                | J√° num√©rico
7     | Gross Sales           | object  | " $32,370.00 "               | String monet√°ria
8     | Discounts             | object  | " $-   " ou " $276.15 "      | "$-" = zero
9     | Sales                 | object  | " $32,370.00 "               | String monet√°ria
10    | COGS                  | object  | " $16,185.00 "               | Custo dos produtos
11    | Profit                | object  | " $16,185.00 "               | Pode ser negativo
12    | Date                  | object  | "01/01/2014"                 | Formato DD/MM/YYYY
13    | Month Number          | int     | 1, 6, 12                     | 1-12
14    | Month Name            | object  | " January ", " June "        | Com espa√ßos
15    | Year                  | int     | 2013, 2014                   | 2 anos apenas
```

**Problemas Identificados**:

- ‚ùå 9 colunas num√©ricas armazenadas como `object` (strings)
- ‚ùå Valores monet√°rios com s√≠mbolos `$` e `,` (mil separadores)
- ‚ùå Espa√ßos em branco no in√≠cio/fim de strings
- ‚ùå Representa√ß√£o inconsistente de zero: `" $-   "` vs `0`
- ‚ùå Datas n√£o convertidas para tipo `datetime`

---

## 3. Etapa 1: Extra√ß√£o de Dados

### 3.1 Fun√ß√£o de Carregamento

```python
import pandas as pd
import numpy as np
from pathlib import Path
import logging

# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def carregar_dados(caminho_arquivo):
    """
    Carrega arquivo CSV com tratamento robusto de encoding e erros.

    Parameters
    ----------
    caminho_arquivo : str ou Path
        Caminho absoluto ou relativo para o arquivo CSV

    Returns
    -------
    pd.DataFrame
        DataFrame com dados brutos do CSV

    Raises
    ------
    FileNotFoundError
        Se o arquivo n√£o existir no caminho especificado
    pd.errors.ParserError
        Se o CSV estiver malformado

    Examples
    --------
    >>> df = carregar_dados("Financials.csv")
    >>> df.shape
    (701, 16)

    Notes
    -----
    - Usa encoding UTF-8 por padr√£o (comum em dados internacionais)
    - on_bad_lines='skip' ignora linhas com n√∫mero incorreto de campos
    - low_memory=False desativa o chunking para infer√™ncia de tipos
    """

    # Valida se o arquivo existe
    arquivo = Path(caminho_arquivo)
    if not arquivo.exists():
        raise FileNotFoundError(f"Arquivo n√£o encontrado: {arquivo.absolute()}")

    logger.info(f"üìÅ Carregando arquivo: {arquivo.name} ({arquivo.stat().st_size / 1024:.1f} KB)")

    try:
        # Par√¢metros otimizados para CSVs financeiros
        df = pd.read_csv(
            arquivo,
            encoding='utf-8',           # Suporta caracteres especiais
            on_bad_lines='skip',        # Ignora linhas corrompidas
            low_memory=False,           # Melhora infer√™ncia de tipos
            skipinitialspace=True,      # Remove espa√ßos ap√≥s delimitador
            na_values=['', 'NA', 'N/A', 'null', 'NULL']  # Define valores nulos
        )

        logger.info(f"‚úÖ Dataset carregado: {df.shape[0]} linhas √ó {df.shape[1]} colunas")

        # An√°lise explorat√≥ria inicial
        logger.info("\nüìä AN√ÅLISE EXPLORAT√ìRIA INICIAL:")
        logger.info(f"   - Tamanho em mem√≥ria: {df.memory_usage(deep=True).sum() / 1024:.1f} KB")
        logger.info(f"   - Colunas: {list(df.columns)}")
        logger.info(f"   - Tipos de dados: {df.dtypes.value_counts().to_dict()}")
        logger.info(f"   - Valores nulos totais: {df.isnull().sum().sum()}")

        # Amostra de dados
        print("\nüîç Primeiras 3 linhas:")
        print(df.head(3))

        return df

    except pd.errors.ParserError as e:
        logger.error(f"‚ùå Erro ao parsear CSV: {str(e)}")
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro inesperado: {str(e)}")
        raise
```

### 3.2 An√°lise Explorat√≥ria Inicial (EDA)

```python
def analise_exploratoria(df):
    """
    Gera relat√≥rio detalhado sobre a qualidade dos dados brutos.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame a ser analisado

    Returns
    -------
    dict
        Dicion√°rio com m√©tricas de qualidade
    """

    print("\n" + "="*70)
    print("üìà RELAT√ìRIO DE AN√ÅLISE EXPLORAT√ìRIA DE DADOS (EDA)")
    print("="*70)

    # 1. Informa√ß√µes Gerais
    print("\n1Ô∏è‚É£ INFORMA√á√ïES GERAIS:")
    print(f"   ‚Ä¢ Shape: {df.shape[0]} linhas √ó {df.shape[1]} colunas")
    print(f"   ‚Ä¢ Mem√≥ria ocupada: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB")

    # 2. Tipos de Dados
    print("\n2Ô∏è‚É£ DISTRIBUI√á√ÉO DE TIPOS:")
    tipo_counts = df.dtypes.value_counts()
    for tipo, count in tipo_counts.items():
        print(f"   ‚Ä¢ {tipo}: {count} colunas")

    # 3. Valores Nulos
    print("\n3Ô∏è‚É£ VALORES NULOS:")
    nulos = df.isnull().sum()
    if nulos.sum() == 0:
        print("   ‚úÖ Nenhum valor nulo detectado!")
    else:
        print(nulos[nulos > 0])

    # 4. Colunas Num√©ricas (deveria ser num√©rico mas est√° como object)
    print("\n4Ô∏è‚É£ COLUNAS NUM√âRICAS MASCARADAS COMO OBJECT:")
    colunas_suspeitas = []
    for col in df.select_dtypes(include='object').columns:
        # Tenta converter 10 primeiros valores n√£o-nulos
        amostra = df[col].dropna().head(10)
        if amostra.str.contains(r'\$|[\d,]+\.?\d*').any():
            colunas_suspeitas.append(col)
            print(f"   ‚ö†Ô∏è {col}: cont√©m valores monet√°rios")
            print(f"      Exemplo: {amostra.iloc[0]}")

    # 5. Duplicatas
    print("\n5Ô∏è‚É£ DUPLICATAS:")
    duplicatas = df.duplicated().sum()
    if duplicatas > 0:
        print(f"   ‚ö†Ô∏è {duplicatas} linhas duplicadas ({duplicatas/len(df)*100:.1f}%)")
    else:
        print("   ‚úÖ Nenhuma duplicata encontrada!")

    # 6. Cardinalidade de Colunas Categ√≥ricas
    print("\n6Ô∏è‚É£ CARDINALIDADE (colunas categ√≥ricas):")
    for col in df.select_dtypes(include='object').columns:
        if col not in colunas_suspeitas:  # Ignora monet√°rias
            nunique = df[col].nunique()
            print(f"   ‚Ä¢ {col}: {nunique} valores √∫nicos")
            if nunique <= 10:
                print(f"      ‚Üí {df[col].value_counts().head(3).to_dict()}")

    # 7. Estat√≠sticas Descritivas (colunas j√° num√©ricas)
    print("\n7Ô∏è‚É£ ESTAT√çSTICAS DESCRITIVAS (colunas num√©ricas puras):")
    print(df.describe(include=[np.number]))

    # Retorna m√©tricas como dicion√°rio
    metricas = {
        'shape': df.shape,
        'memoria_mb': df.memory_usage(deep=True).sum() / (1024**2),
        'valores_nulos': df.isnull().sum().sum(),
        'duplicatas': duplicatas,
        'colunas_monetarias': colunas_suspeitas
    }

    return metricas
```

### 3.3 Exemplo de Uso

```python
# Executar extra√ß√£o
df_bruto = carregar_dados("Financials.csv")
metricas_iniciais = analise_exploratoria(df_bruto)

# Resultado esperado:
# =====================================================================
# üìÅ Carregando arquivo: Financials.csv (119.0 KB)
# ‚úÖ Dataset carregado: 701 linhas √ó 16 colunas
#
# üìä AN√ÅLISE EXPLORAT√ìRIA INICIAL:
#    - Tamanho em mem√≥ria: 89.2 KB
#    - Colunas: ['Segment', 'Country', 'Product', ...]
#    - Tipos de dados: {'object': 12, 'int64': 2, 'float64': 2}
#    - Valores nulos totais: 0
# =====================================================================
```

---

## 4. Etapa 2: Limpeza e Valida√ß√£o

### 4.1 Estrat√©gias de Limpeza

#### 4.1.1 Remo√ß√£o de Espa√ßos em Branco

**Problema**: Strings como `" January "` causam falhas em joins e agrupamentos.

**Solu√ß√£o**:

```python
def limpar_espacos(df):
    """
    Remove espa√ßos em branco do in√≠cio/fim de todas as strings.

    T√©cnica: Aplica .str.strip() em todas as colunas object.
    Complexidade: O(n*m) onde n=linhas, m=colunas de texto
    """
    df_clean = df.copy()

    # Remove espa√ßos dos nomes das colunas
    df_clean.columns = df_clean.columns.str.strip()

    # Remove espa√ßos dos valores
    colunas_texto = df_clean.select_dtypes(include='object').columns

    for col in colunas_texto:
        # .str.strip() √© vetorizado (r√°pido)
        df_clean[col] = df_clean[col].str.strip()

    logger.info(f"üßπ Espa√ßos removidos de {len(colunas_texto)} colunas de texto")

    return df_clean
```

**Performance**:

- Dataset de 700 linhas: ~5ms
- Dataset de 1M linhas: ~2s
- Alternativa mais r√°pida: usar `apply(lambda x: x.strip() if isinstance(x, str) else x)` com `numba`

#### 4.1.2 Deduplica√ß√£o

**Problema**: Linhas id√™nticas inflacionam m√©tricas agregadas.

**Solu√ß√£o**:

```python
def remover_duplicatas(df, subset=None, keep='first'):
    """
    Remove linhas duplicadas com estrat√©gia configur√°vel.

    Parameters
    ----------
    df : pd.DataFrame
    subset : list, optional
        Colunas para considerar na compara√ß√£o (None = todas)
    keep : {'first', 'last', False}
        'first': mant√©m primeira ocorr√™ncia
        'last': mant√©m √∫ltima ocorr√™ncia
        False: remove todas as duplicatas

    Returns
    -------
    pd.DataFrame
        DataFrame sem duplicatas

    Notes
    -----
    Pandas usa hashing interno para detectar duplicatas:
    - Complexidade: O(n) em m√©dia
    - Usa ~2x mem√≥ria durante execu√ß√£o
    """
    linhas_antes = len(df)

    # drop_duplicates √© otimizado com hash table
    df_dedup = df.drop_duplicates(subset=subset, keep=keep)

    linhas_depois = len(df_dedup)
    removidas = linhas_antes - linhas_depois

    if removidas > 0:
        logger.warning(
            f"‚ö†Ô∏è {removidas} duplicatas removidas "
            f"({removidas/linhas_antes*100:.2f}%)"
        )
    else:
        logger.info("‚úÖ Nenhuma duplicata encontrada")

    return df_dedup
```

**Estrat√©gias Avan√ßadas**:

```python
# 1. Deduplica√ß√£o por chave de neg√≥cio
# (em vez de comparar todas as colunas)
df_dedup = remover_duplicatas(
    df,
    subset=['Country', 'Product', 'Date', 'Segment'],
    keep='last'  # Mant√©m transa√ß√£o mais recente
)

# 2. Deduplica√ß√£o "fuzzy" (para dados com typos)
from fuzzywuzzy import fuzz

def dedup_fuzzy(df, coluna, threshold=90):
    """Remove duplicatas com similaridade > threshold%"""
    grupos = []
    visitados = set()

    for i, val1 in enumerate(df[coluna]):
        if i in visitados:
            continue
        grupo = [i]
        for j, val2 in enumerate(df[coluna].iloc[i+1:], start=i+1):
            if fuzz.ratio(val1, val2) >= threshold:
                grupo.append(j)
                visitados.add(j)
        grupos.append(grupo)

    # Mant√©m primeira ocorr√™ncia de cada grupo
    indices_manter = [g[0] for g in grupos]
    return df.iloc[indices_manter]
```

#### 4.1.3 Tratamento de Valores Nulos

**Estrat√©gias Por Tipo de Dado**:

```python
def tratar_nulos(df):
    """
    Imputa valores nulos com estrat√©gias espec√≠ficas por coluna.

    Regras:
    - Num√©ricos cont√≠nuos: mediana (robusta a outliers)
    - Num√©ricos discretos: moda (valor mais frequente)
    - Categ√≥ricos: categoria "Unknown"
    - Datas: forward fill (√∫ltima data v√°lida)
    """
    df_filled = df.copy()

    # 1. Colunas num√©ricas cont√≠nuas (ex: Sales, Profit)
    colunas_continuas = ['Gross Sales', 'Sales', 'Profit', 'COGS', 'Discounts']
    for col in colunas_continuas:
        if col in df_filled.columns and df_filled[col].dtype in ['float64', 'int64']:
            mediana = df_filled[col].median()
            nulos_antes = df_filled[col].isnull().sum()
            df_filled[col] = df_filled[col].fillna(mediana)
            if nulos_antes > 0:
                logger.info(f"   ‚Ä¢ {col}: {nulos_antes} nulos ‚Üí mediana ({mediana:.2f})")

    # 2. Colunas num√©ricas discretas (ex: Units Sold)
    colunas_discretas = ['Units Sold', 'Month Number']
    for col in colunas_discretas:
        if col in df_filled.columns and df_filled[col].dtype in ['float64', 'int64']:
            moda = df_filled[col].mode()[0]
            nulos_antes = df_filled[col].isnull().sum()
            df_filled[col] = df_filled[col].fillna(moda)
            if nulos_antes > 0:
                logger.info(f"   ‚Ä¢ {col}: {nulos_antes} nulos ‚Üí moda ({moda})")

    # 3. Colunas categ√≥ricas
    colunas_categoricas = ['Segment', 'Country', 'Product', 'Discount Band']
    for col in colunas_categoricas:
        if col in df_filled.columns:
            nulos_antes = df_filled[col].isnull().sum()
            df_filled[col] = df_filled[col].fillna('Unknown')
            if nulos_antes > 0:
                logger.info(f"   ‚Ä¢ {col}: {nulos_antes} nulos ‚Üí 'Unknown'")

    # 4. Colunas de data
    if 'Date' in df_filled.columns:
        nulos_antes = df_filled['Date'].isnull().sum()
        df_filled['Date'] = df_filled['Date'].fillna(method='ffill')  # Forward fill
        if nulos_antes > 0:
            logger.info(f"   ‚Ä¢ Date: {nulos_antes} nulos ‚Üí forward fill")

    return df_filled
```

**T√©cnicas Avan√ßadas de Imputa√ß√£o**:

```python
from sklearn.impute import KNNImputer, IterativeImputer

# 1. KNN Imputer (usa K vizinhos mais pr√≥ximos)
imputer_knn = KNNImputer(n_neighbors=5)
df[['Sales', 'Profit']] = imputer_knn.fit_transform(df[['Sales', 'Profit']])

# 2. MICE (Multiple Imputation by Chained Equations)
imputer_mice = IterativeImputer(random_state=42)
df_imputed = pd.DataFrame(
    imputer_mice.fit_transform(df.select_dtypes(include=[np.number])),
    columns=df.select_dtypes(include=[np.number]).columns
)
```

---

## 5. Etapa 3: Transforma√ß√£o de Tipos

### 5.1 Convers√£o de Strings Monet√°rias

**Desafio**: Converter `" $1,618.50 "` ‚Üí `1618.50` (float)

```python
def converter_monetario(df, colunas_monetarias):
    """
    Converte strings monet√°rias para float64.

    Trata os seguintes formatos:
    - " $1,234.56 "   ‚Üí 1234.56
    - " $-   "        ‚Üí 0.0
    - "(1,234.56)"    ‚Üí -1234.56 (par√™nteses = negativo em finan√ßas)
    - "1234"          ‚Üí 1234.0

    Parameters
    ----------
    df : pd.DataFrame
    colunas_monetarias : list
        Lista de colunas a converter

    Returns
    -------
    pd.DataFrame
        DataFrame com colunas convertidas

    Performance
    -----------
    - Regex: ~100ms para 1M linhas
    - Alternativa: usar replace() m√∫ltiplo (~30ms)
    """
    df_converted = df.copy()

    for col in colunas_monetarias:
        if col not in df_converted.columns:
            logger.warning(f"‚ö†Ô∏è Coluna '{col}' n√£o encontrada, pulando...")
            continue

        if df_converted[col].dtype == 'object':
            # Pipeline de limpeza
            serie = df_converted[col]

            # Etapa 1: Remove s√≠mbolos e espa√ßos
            serie = serie.str.replace('$', '', regex=False)
            serie = serie.str.replace(',', '', regex=False)
            serie = serie.str.replace(' ', '', regex=False)
            serie = serie.str.replace('"', '', regex=False)

            # Etapa 2: Trata valores especiais
            serie = serie.replace('-', '0')  # "$-" significa zero
            serie = serie.replace('', '0')   # String vazia = zero

            # Etapa 3: Trata valores negativos em par√™nteses
            # Ex: "(1234.56)" ‚Üí "-1234.56"
            serie = serie.str.replace(r'^\((.*)\)$', r'-\1', regex=True)

            # Etapa 4: Converte para float
            df_converted[col] = pd.to_numeric(serie, errors='coerce').fillna(0)

            logger.info(f"üí± {col}: convertido para float64 (range: {df_converted[col].min():.2f} a {df_converted[col].max():.2f})")

    return df_converted
```

**M√©todo Alternativo (Mais R√°pido)**:

```python
import re

def converter_monetario_rapido(valor):
    """
    Converte um √∫nico valor monet√°rio usando regex.
    5x mais r√°pido que m√∫ltiplos .str.replace()
    """
    if pd.isna(valor) or valor == '-':
        return 0.0

    # Remove tudo exceto d√≠gitos, ponto e sinal negativo
    limpo = re.sub(r'[^\d.-]', '', str(valor))

    try:
        return float(limpo) if limpo else 0.0
    except ValueError:
        return 0.0

# Aplica√ß√£o vetorizada
df['Sales'] = df['Sales'].apply(converter_monetario_rapido)
```

### 5.2 Convers√£o de Datas

```python
def converter_datas(df, coluna_data='Date', formato='%d/%m/%Y'):
    """
    Converte string de data para datetime64[ns].

    Formatos suportados:
    - DD/MM/YYYY  ‚Üí  2014-06-01
    - MM-DD-YYYY  ‚Üí  2014-06-01
    - YYYY/MM/DD  ‚Üí  2014-06-01
    - Auto-detect ‚Üí  pd.to_datetime com infer_datetime_format

    Parameters
    ----------
    df : pd.DataFrame
    coluna_data : str
        Nome da coluna com datas
    formato : str, optional
        Formato strptime (None = auto-detect)

    Returns
    -------
    pd.DataFrame
        DataFrame com coluna convertida

    Notes
    -----
    Auto-detect √© ~10x mais lento que especificar formato expl√≠cito
    """
    df_converted = df.copy()

    if coluna_data not in df_converted.columns:
        raise ValueError(f"Coluna '{coluna_data}' n√£o encontrada no DataFrame")

    try:
        if formato:
            # Convers√£o com formato expl√≠cito (R√ÅPIDO)
            df_converted[coluna_data] = pd.to_datetime(
                df_converted[coluna_data],
                format=formato,
                errors='coerce'  # Valores inv√°lidos viram NaT
            )
        else:
            # Auto-detect (LENTO mas flex√≠vel)
            df_converted[coluna_data] = pd.to_datetime(
                df_converted[coluna_data],
                infer_datetime_format=True,
                errors='coerce'
            )

        # Estat√≠sticas da convers√£o
        nat_count = df_converted[coluna_data].isna().sum()
        if nat_count > 0:
            logger.warning(f"‚ö†Ô∏è {nat_count} datas inv√°lidas convertidas para NaT")

        data_min = df_converted[coluna_data].min()
        data_max = df_converted[coluna_data].max()
        logger.info(f"üìÖ {coluna_data}: {data_min.date()} a {data_max.date()} ({(data_max - data_min).days} dias)")

    except Exception as e:
        logger.error(f"‚ùå Erro ao converter datas: {str(e)}")
        raise

    return df_converted
```

### 5.3 Otimiza√ß√£o de Tipos de Dados

**Reduzir uso de mem√≥ria em 50-90%**:

```python
def otimizar_tipos(df):
    """
    Converte tipos de dados para vers√µes mais eficientes.

    Otimiza√ß√µes:
    - int64 ‚Üí int32 (se range permitir)
    - object ‚Üí category (se cardinalidade < 50%)
    - float64 ‚Üí float32 (perda m√≠nima de precis√£o)

    Returns
    -------
    pd.DataFrame
        DataFrame otimizado

    Examples
    --------
    >>> df_original.memory_usage(deep=True).sum()
    1048576  # 1 MB
    >>> df_otimizado = otimizar_tipos(df_original)
    >>> df_otimizado.memory_usage(deep=True).sum()
    262144  # 256 KB (redu√ß√£o de 75%)
    """
    df_opt = df.copy()
    mem_antes = df_opt.memory_usage(deep=True).sum() / 1024**2

    # 1. Converte object ‚Üí category (se cardinalidade baixa)
    for col in df_opt.select_dtypes(include='object').columns:
        nunique = df_opt[col].nunique()
        total = len(df_opt[col])

        if nunique / total < 0.5:  # < 50% de valores √∫nicos
            df_opt[col] = df_opt[col].astype('category')
            logger.info(f"   ‚Ä¢ {col}: object ‚Üí category ({nunique} categorias)")

    # 2. Converte int64 ‚Üí int32 (se valores cabem)
    for col in df_opt.select_dtypes(include='int64').columns:
        col_min = df_opt[col].min()
        col_max = df_opt[col].max()

        # Limites do int32: -2,147,483,648 a 2,147,483,647
        if col_min >= -2147483648 and col_max <= 2147483647:
            df_opt[col] = df_opt[col].astype('int32')
            logger.info(f"   ‚Ä¢ {col}: int64 ‚Üí int32")

    # 3. Converte float64 ‚Üí float32 (CUIDADO: pode haver perda de precis√£o)
    # S√≥ recomendado se n√£o for fazer c√°lculos financeiros cr√≠ticos
    # for col in df_opt.select_dtypes(include='float64').columns:
    #     df_opt[col] = df_opt[col].astype('float32')

    mem_depois = df_opt.memory_usage(deep=True).sum() / 1024**2
    reducao = (1 - mem_depois / mem_antes) * 100

    logger.info(f"\nüíæ OTIMIZA√á√ÉO DE MEM√ìRIA:")
    logger.info(f"   Antes:  {mem_antes:.2f} MB")
    logger.info(f"   Depois: {mem_depois:.2f} MB")
    logger.info(f"   Redu√ß√£o: {reducao:.1f}%")

    return df_opt
```

---

## 6. Etapa 4: Feature Engineering

### 6.1 Cria√ß√£o de M√©tricas Financeiras

```python
def criar_metricas_financeiras(df):
    """
    Calcula KPIs financeiros derivados.

    M√©tricas criadas:
    1. Profit_Margin_% = (Profit / Sales) √ó 100
    2. Discount_Rate_% = (Discounts / Gross Sales) √ó 100
    3. COGS_Ratio_% = (COGS / Sales) √ó 100
    4. Revenue_Per_Unit = Sales / Units Sold
    5. Markup_% = ((Sale Price - Manuf. Price) / Manuf. Price) √ó 100
    6. ROI_% = (Profit / COGS) √ó 100

    Parameters
    ----------
    df : pd.DataFrame
        Deve conter: Sales, Profit, Gross Sales, Discounts, etc.

    Returns
    -------
    pd.DataFrame
        DataFrame com 6 colunas adicionais

    Notes
    -----
    Usa np.where para evitar divis√£o por zero
    """
    df_eng = df.copy()

    # 1. Margem de Lucro (%)
    # F√≥rmula: Lucro / Receita L√≠quida
    # Interpreta√ß√£o: 40% = ganhamos $0.40 para cada $1 vendido
    df_eng['Profit_Margin_%'] = np.where(
        df_eng['Sales'] != 0,
        (df_eng['Profit'] / df_eng['Sales']) * 100,
        0
    )

    # 2. Taxa de Desconto Efetiva (%)
    # Quanto % do pre√ßo bruto foi descontado
    df_eng['Discount_Rate_%'] = np.where(
        df_eng['Gross Sales'] != 0,
        (df_eng['Discounts'] / df_eng['Gross Sales']) * 100,
        0
    )

    # 3. Raz√£o de Custo (%)
    # Quanto % da receita √© custo do produto
    df_eng['COGS_Ratio_%'] = np.where(
        df_eng['Sales'] != 0,
        (df_eng['COGS'] / df_eng['Sales']) * 100,
        0
    )

    # 4. Receita por Unidade
    # Pre√ßo m√©dio efetivo por unidade vendida
    df_eng['Revenue_Per_Unit'] = np.where(
        df_eng['Units Sold'] != 0,
        df_eng['Sales'] / df_eng['Units Sold'],
        0
    )

    # 5. Markup sobre Custo de Fabrica√ß√£o (%)
    # Quanto % foi marcado acima do custo
    df_eng['Markup_%'] = np.where(
        df_eng['Manufacturing Price'] != 0,
        ((df_eng['Sale Price'] - df_eng['Manufacturing Price']) /
         df_eng['Manufacturing Price']) * 100,
        0
    )

    # 6. Retorno sobre Custo (ROI)
    # Quanto de lucro para cada $1 de custo
    df_eng['ROI_%'] = np.where(
        df_eng['COGS'] != 0,
        (df_eng['Profit'] / df_eng['COGS']) * 100,
        0
    )

    logger.info("‚ûï 6 m√©tricas financeiras criadas:")
    logger.info("   1. Profit_Margin_% (margem de lucro)")
    logger.info("   2. Discount_Rate_% (taxa de desconto)")
    logger.info("   3. COGS_Ratio_% (custo/receita)")
    logger.info("   4. Revenue_Per_Unit (receita unit√°ria)")
    logger.info("   5. Markup_% (marca√ß√£o de pre√ßo)")
    logger.info("   6. ROI_% (retorno sobre custo)")

    return df_eng
```

### 6.2 Engenharia Temporal

```python
def criar_features_temporais(df, coluna_data='Date'):
    """
    Extrai features temporais de uma coluna datetime.

    Features criadas:
    - Quarter: 1, 2, 3, 4
    - Year_Quarter: "2014-Q1"
    - DayOfWeek: 0 (Monday) a 6 (Sunday)
    - DayOfWeek_Name: "Monday", "Tuesday", ...
    - IsWeekend: True/False
    - WeekOfYear: 1 a 52
    - DaysFromStart: dias desde primeira transa√ß√£o
    """
    df_temporal = df.copy()

    if coluna_data not in df_temporal.columns:
        raise ValueError(f"Coluna '{coluna_data}' n√£o encontrada")

    if df_temporal[coluna_data].dtype != 'datetime64[ns]':
        raise TypeError(f"Coluna '{coluna_data}' deve ser datetime64, n√£o {df_temporal[coluna_data].dtype}")

    # 1. Trimestre
    df_temporal['Quarter'] = df_temporal[coluna_data].dt.quarter

    # 2. Ano-Trimestre (√∫til para s√©ries temporais)
    df_temporal['Year_Quarter'] = (
        df_temporal[coluna_data].dt.year.astype(str) +
        '-Q' +
        df_temporal['Quarter'].astype(str)
    )

    # 3. Dia da Semana
    df_temporal['DayOfWeek'] = df_temporal[coluna_data].dt.dayofweek
    df_temporal['DayOfWeek_Name'] = df_temporal[coluna_data].dt.day_name()

    # 4. √â Final de Semana?
    df_temporal['IsWeekend'] = df_temporal['DayOfWeek'].isin([5, 6])

    # 5. Semana do Ano
    df_temporal['WeekOfYear'] = df_temporal[coluna_data].dt.isocalendar().week

    # 6. Dias desde a primeira transa√ß√£o
    data_inicial = df_temporal[coluna_data].min()
    df_temporal['DaysFromStart'] = (df_temporal[coluna_data] - data_inicial).dt.days

    logger.info("üìÖ 7 features temporais criadas:")
    logger.info("   ‚Ä¢ Quarter, Year_Quarter")
    logger.info("   ‚Ä¢ DayOfWeek, DayOfWeek_Name, IsWeekend")
    logger.info("   ‚Ä¢ WeekOfYear, DaysFromStart")

    return df_temporal
```

### 6.3 Binning e Categoriza√ß√£o

```python
def criar_categorias(df):
    """
    Cria categorias por binning de vari√°veis cont√≠nuas.

    Categorias criadas:
    - Volume_Category: Baixo/M√©dio/Alto (baseado em Units Sold)
    - Price_Tier: Economy/Standard/Premium (baseado em Sale Price)
    - Profitability_Label: Loss/Break-even/Low/Medium/High
    """
    df_cat = df.copy()

    # 1. Categoria de Volume de Vendas
    # Bins definidos por quantis (33%, 66%)
    df_cat['Volume_Category'] = pd.qcut(
        df_cat['Units Sold'],
        q=3,
        labels=['Baixo Volume', 'M√©dio Volume', 'Alto Volume'],
        duplicates='drop'  # Se houver valores repetidos nos cortes
    )

    # 2. N√≠vel de Pre√ßo
    # Bins fixos baseados em conhecimento do neg√≥cio
    df_cat['Price_Tier'] = pd.cut(
        df_cat['Sale Price'],
        bins=[0, 15, 50, float('inf')],
        labels=['Economy', 'Standard', 'Premium']
    )

    # 3. Categoria de Lucratividade
    # Baseado em margem de lucro
    def classificar_lucratividade(margem):
        if margem < 0:
            return 'Loss'
        elif margem == 0:
            return 'Break-even'
        elif margem < 20:
            return 'Low Profit'
        elif margem < 40:
            return 'Medium Profit'
        else:
            return 'High Profit'

    df_cat['Profitability_Label'] = df_cat['Profit_Margin_%'].apply(classificar_lucratividade)

    logger.info("üè∑Ô∏è 3 categorias criadas por binning:")
    logger.info(f"   ‚Ä¢ Volume_Category: {df_cat['Volume_Category'].value_counts().to_dict()}")
    logger.info(f"   ‚Ä¢ Price_Tier: {df_cat['Price_Tier'].value_counts().to_dict()}")
    logger.info(f"   ‚Ä¢ Profitability_Label: {df_cat['Profitability_Label'].value_counts().to_dict()}")

    return df_cat
```

---

## 7. Etapa 5: Valida√ß√£o de Qualidade

### 7.1 Testes de Integridade

```python
def validar_integridade_dados(df):
    """
    Executa 10 testes de integridade em dados financeiros.

    Testes:
    1. Sales = Gross Sales - Discounts
    2. Profit = Sales - COGS
    3. Valores negativos inesperados
    4. Outliers (m√©todo IQR)
    5. Consist√™ncia de datas
    6. Margens imposs√≠veis (ex: > 100%)
    7. Pre√ßos zero
    8. Unidades vendidas zero
    9. Relacionamento Manufacturing Price < Sale Price
    10. Soma de percentuais (COGS% + Margin% ‚âà 100%)

    Returns
    -------
    dict
        Relat√≥rio com falhas e avisos
    """
    relatorio = {
        'testes_passados': 0,
        'testes_falhados': 0,
        'avisos': []
    }

    print("\n" + "="*70)
    print("üîç VALIDA√á√ÉO DE INTEGRIDADE DE DADOS")
    print("="*70)

    # Teste 1: Sales = Gross Sales - Discounts
    print("\n1Ô∏è‚É£ Teste: Sales = Gross Sales - Discounts")
    df['Calc_Sales'] = df['Gross Sales'] - df['Discounts']
    desvios = abs(df['Sales'] - df['Calc_Sales']) > 0.01  # Toler√¢ncia de 1 centavo
    if desvios.sum() > 0:
        relatorio['testes_falhados'] += 1
        relatorio['avisos'].append(f"‚ùå {desvios.sum()} transa√ß√µes com Sales inconsistente")
        print(f"   ‚ùå FALHOU: {desvios.sum()} inconsist√™ncias detectadas")
    else:
        relatorio['testes_passados'] += 1
        print("   ‚úÖ PASSOU")
    df.drop('Calc_Sales', axis=1, inplace=True)

    # Teste 2: Profit = Sales - COGS
    print("\n2Ô∏è‚É£ Teste: Profit = Sales - COGS")
    df['Calc_Profit'] = df['Sales'] - df['COGS']
    desvios = abs(df['Profit'] - df['Calc_Profit']) > 0.01
    if desvios.sum() > 0:
        relatorio['testes_falhados'] += 1
        relatorio['avisos'].append(f"‚ùå {desvios.sum()} transa√ß√µes com Profit inconsistente")
        print(f"   ‚ùå FALHOU: {desvios.sum()} inconsist√™ncias")
    else:
        relatorio['testes_passados'] += 1
        print("   ‚úÖ PASSOU")
    df.drop('Calc_Profit', axis=1, inplace=True)

    # Teste 3: Valores Negativos Inesperados
    print("\n3Ô∏è‚É£ Teste: Valores negativos inesperados")
    colunas_positivas = ['Units Sold', 'Gross Sales', 'Sales', 'COGS']
    tem_negativo = False
    for col in colunas_positivas:
        if col in df.columns:
            negativos = (df[col] < 0).sum()
            if negativos > 0:
                tem_negativo = True
                relatorio['avisos'].append(f"‚ö†Ô∏è {negativos} valores negativos em '{col}'")
                print(f"   ‚ö†Ô∏è {col}: {negativos} valores negativos")

    if tem_negativo:
        relatorio['testes_falhados'] += 1
    else:
        relatorio['testes_passados'] += 1
        print("   ‚úÖ PASSOU")

    # Teste 4: Outliers (m√©todo IQR)
    print("\n4Ô∏è‚É£ Teste: Detec√ß√£o de outliers (IQR)")
    colunas_numericas = df.select_dtypes(include=[np.number]).columns
    for col in colunas_numericas[:5]:  # Primeiras 5 colunas num√©ricas
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        limite_inferior = Q1 - 1.5 * IQR
        limite_superior = Q3 + 1.5 * IQR

        outliers = ((df[col] < limite_inferior) | (df[col] > limite_superior)).sum()
        if outliers > 0:
            print(f"   ‚ö†Ô∏è {col}: {outliers} outliers ({outliers/len(df)*100:.1f}%)")
    relatorio['testes_passados'] += 1

    # Teste 5: Margens imposs√≠veis
    print("\n5Ô∏è‚É£ Teste: Margens > 100%")
    if 'Profit_Margin_%' in df.columns:
        margens_altas = (df['Profit_Margin_%'] > 100).sum()
        if margens_altas > 0:
            relatorio['avisos'].append(f"‚ö†Ô∏è {margens_altas} transa√ß√µes com margem > 100%")
            print(f"   ‚ö†Ô∏è {margens_altas} margens acima de 100% (verificar se √© esperado)")
        else:
            print("   ‚úÖ PASSOU")
    relatorio['testes_passados'] += 1

    # Resumo
    print("\n" + "="*70)
    print("üìä RESUMO DA VALIDA√á√ÉO:")
    print(f"   ‚úÖ Testes Passados: {relatorio['testes_passados']}")
    print(f"   ‚ùå Testes Falhados: {relatorio['testes_falhados']}")
    print(f"   ‚ö†Ô∏è Avisos: {len(relatorio['avisos'])}")
    if relatorio['avisos']:
        print("\n   Detalhes dos Avisos:")
        for aviso in relatorio['avisos']:
            print(f"      {aviso}")
    print("="*70)

    return relatorio
```

---

## 8. Otimiza√ß√£o e Performance

### 8.1 T√©cnicas de Otimiza√ß√£o

```python
# 1. Chunking para arquivos grandes (> 1GB)
def processar_em_chunks(arquivo, tamanho_chunk=10000):
    """
    Processa CSV em blocos para economizar mem√≥ria.
    √ötil para datasets > 1GB.
    """
    chunks = []
    for chunk in pd.read_csv(arquivo, chunksize=tamanho_chunk):
        # Processa cada chunk
        chunk = limpar_dados(chunk)
        chunk = converter_tipos(chunk)
        chunks.append(chunk)

    # Concatena todos os chunks
    df_completo = pd.concat(chunks, ignore_index=True)
    return df_completo

# 2. Paraleliza√ß√£o com multiprocessing
from multiprocessing import Pool

def processar_paralelo(df, funcao, n_cores=4):
    """
    Divide DataFrame em n_cores partes e processa em paralelo.
    """
    df_split = np.array_split(df, n_cores)
    with Pool(n_cores) as pool:
        df_processado = pd.concat(pool.map(funcao, df_split))
    return df_processado

# 3. Vetoriza√ß√£o vs Loops
# ‚ùå LENTO (loop Python)
for i, row in df.iterrows():
    df.at[i, 'Margin'] = row['Profit'] / row['Sales']

# ‚úÖ R√ÅPIDO (vetorizado)
df['Margin'] = df['Profit'] / df['Sales']
```

---

## 9. Tratamento de Erros

```python
class ETLError(Exception):
    """Exce√ß√£o base para erros de ETL"""
    pass

class DataValidationError(ETLError):
    """Erro de valida√ß√£o de dados"""
    pass

class TransformationError(ETLError):
    """Erro durante transforma√ß√£o"""
    pass

def pipeline_etl_robusto(arquivo_entrada, arquivo_saida):
    """
    Pipeline ETL com tratamento completo de erros.
    """
    try:
        # Etapa 1: Extra√ß√£o
        try:
            df = carregar_dados(arquivo_entrada)
        except FileNotFoundError:
            logger.critical(f"Arquivo n√£o encontrado: {arquivo_entrada}")
            raise
        except pd.errors.ParserError as e:
            logger.critical(f"Erro ao parsear CSV: {e}")
            raise ETLError(f"CSV malformado: {e}")

        # Etapa 2: Transforma√ß√£o
        try:
            df = limpar_dados(df)
            df = converter_tipos(df)
            df = criar_metricas_financeiras(df)
        except Exception as e:
            logger.error(f"Erro na transforma√ß√£o: {e}")
            raise TransformationError(f"Falha ao transformar dados: {e}")

        # Etapa 3: Valida√ß√£o
        relatorio = validar_integridade_dados(df)
        if relatorio['testes_falhados'] > 0:
            logger.warning("Dados com problemas de integridade, mas continuando...")

        # Etapa 4: Carga
        try:
            df.to_csv(arquivo_saida, index=False, encoding='utf-8')
            logger.info(f"‚úÖ Dados salvos em: {arquivo_saida}")
        except PermissionError:
            logger.critical(f"Sem permiss√£o para escrever em: {arquivo_saida}")
            raise

        return df

    except Exception as e:
        logger.critical(f"‚ùå PIPELINE FALHOU: {e}")
        raise
```

---

## 10. Boas Pr√°ticas e Troubleshooting

### 10.1 Checklist de Boas Pr√°ticas

- ‚úÖ **Sempre validar entrada**: Verificar se arquivo existe, se tem colunas esperadas
- ‚úÖ **Logging detalhado**: Usar `logging` em vez de `print()`
- ‚úÖ **Versionamento de dados**: Salvar dados brutos + processados com timestamp
- ‚úÖ **Documenta√ß√£o de transforma√ß√µes**: Comentar regras de neg√≥cio complexas
- ‚úÖ **Testes unit√°rios**: Testar fun√ß√µes cr√≠ticas com `pytest`
- ‚úÖ **Backup antes de sobrescrever**: Copiar arquivo original antes de processamento
- ‚úÖ **Monitoramento de performance**: Usar `%%timeit` em Jupyter para otimizar gargalos

### 10.2 Troubleshooting Comum

| Erro                                            | Causa                            | Solu√ß√£o                                          |
| ----------------------------------------------- | -------------------------------- | ------------------------------------------------ |
| `UnicodeDecodeError`                            | Encoding incorreto               | Usar `encoding='latin-1'` ou `encoding='cp1252'` |
| `MemoryError`                                   | Dataset muito grande             | Usar chunking ou Dask                            |
| `ValueError: could not convert string to float` | Strings com caracteres especiais | Limpar strings antes de `pd.to_numeric()`        |
| `KeyError: 'Coluna'`                            | Nome de coluna errado            | Usar `df.columns.tolist()` para verificar        |
| Performance lenta                               | Loops Python                     | Substituir por opera√ß√µes vetorizadas             |

---

## üìñ Refer√™ncias

1. **Pandas Documentation**: https://pandas.pydata.org/docs/
2. **Livro: Python for Data Analysis** (Wes McKinney)
3. **Real Python - Pandas Tricks**: https://realpython.com/fast-flexible-pandas/
4. **ETL Best Practices**: https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/

---

**Pr√≥ximo Documento**: [FASE_2_INSIGHTS.md](FASE_2_INSIGHTS.md)
